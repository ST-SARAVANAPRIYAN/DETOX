{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314fe4fc",
   "metadata": {},
   "source": [
    "# Detox - Chat Message Toxicity Detector\n",
    "## ADI1302-SPARK SCALA FUNDAMENTALS\n",
    "\n",
    "**Student Name:** SARAVANA PRIYAN S T  \n",
    "**Registration Number:** 927623BAD100\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides an interactive environment for training and analyzing the toxicity detection model using PySpark.\n",
    "\n",
    "### Project Overview\n",
    "- **Objective:** Detect toxic messages in chat data using PySpark and Machine Learning\n",
    "- **Tech Stack:** PySpark, Spark MLlib, Logistic Regression\n",
    "- **Dataset:** Jigsaw Toxic Comments Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0a828",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, when, length\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f2306",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session\n",
    "\n",
    "Configure and create a Spark session with optimized settings for toxicity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933171c3",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session with Web UI enabled\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Detox-Toxicity-Detector-Notebook\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"âœ“ Spark Session Created\")\n",
    "print(f\"  - Spark Version: {spark.version}\")\n",
    "print(f\"  - Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"  - Spark Web UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d62cb5",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Load the toxic comments dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5be4f",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = \"data/chat_data.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    data_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Display schema\n",
    "print(\"Dataset Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c3962",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample Data:\")\n",
    "df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f18f0e4",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Column Names:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Check for null values\n",
    "print(\"\\nNull Value Counts:\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423134d",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Clean and prepare the text data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcf491",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Import preprocessing modules\n",
    "import sys\n",
    "sys.path.append('/home/saravana/projects/ssfproject')\n",
    "\n",
    "from preprocessing import TextPreprocessor\n",
    "from data_ingestion import DataIngestion\n",
    "\n",
    "# Initialize components\n",
    "data_ingestion = DataIngestion(spark)\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Validate data\n",
    "df_clean = data_ingestion.validate_data(df)\n",
    "\n",
    "# Get statistics\n",
    "stats = data_ingestion.get_data_statistics(df_clean)\n",
    "\n",
    "print(f\"âœ“ Data cleaned and validated\")\n",
    "print(f\"  - Clean records: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8056db1",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "df_processed = preprocessor.preprocess_text(df_clean)\n",
    "df_processed = preprocessor.create_label_column(df_processed, label_col=\"toxic\")\n",
    "\n",
    "# Show sample processed data\n",
    "print(\"Processed Data Sample:\")\n",
    "df_processed.select(\"comment_text\", \"cleaned_text\", \"label\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9b43c",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Extract features using TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976d982",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Build and fit feature pipeline\n",
    "df_features = preprocessor.fit_transform_features(df_processed)\n",
    "\n",
    "print(\"âœ“ Features extracted successfully\")\n",
    "print(\"\\nFeature columns:\")\n",
    "print(df_features.columns)\n",
    "\n",
    "# Cache for performance\n",
    "df_features.cache()\n",
    "print(f\"\\nâœ“ Dataset cached ({df_features.count()} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f7d6a",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split\n",
    "\n",
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e0966",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"âœ“ Data split completed\")\n",
    "print(f\"  - Training set: {train_count} records ({train_count/(train_count+test_count)*100:.1f}%)\")\n",
    "print(f\"  - Test set: {test_count} records ({test_count/(train_count+test_count)*100:.1f}%)\")\n",
    "\n",
    "# Cache splits\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39be8ac",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "Train a Logistic Regression model for toxicity classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45a547",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "from model import ToxicityClassifier\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = ToxicityClassifier()\n",
    "\n",
    "# Train model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "model = classifier.train_model(train_df)\n",
    "\n",
    "print(\"\\nâœ“ Model training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090421e",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate the model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3cc6e",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "metrics = classifier.evaluate_model(test_df)\n",
    "\n",
    "# Visualize metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['AUC-ROC', 'AUC-PR', 'Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Score': [\n",
    "        metrics['auc'],\n",
    "        metrics['auc_pr'],\n",
    "        metrics['accuracy'],\n",
    "        metrics['precision'],\n",
    "        metrics['recall'],\n",
    "        metrics['f1_score']\n",
    "    ]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=metrics_df, x='Metric', y='Score', palette='viridis')\n",
    "plt.title('Model Performance Metrics', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xlabel('Metric', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(metrics_df['Score']):\n",
    "    plt.text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Model evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc7a47",
   "metadata": {},
   "source": [
    "## 8. Make Predictions\n",
    "\n",
    "Generate predictions for all messages in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444c45c",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_df = classifier.predict(df_features)\n",
    "\n",
    "print(\"âœ“ Predictions generated\")\n",
    "print(\"\\nPrediction Sample:\")\n",
    "predictions_df.select(\n",
    "    \"id\", \n",
    "    \"comment_text\", \n",
    "    \"toxicity_score\", \n",
    "    \"toxicity_level\", \n",
    "    \"prediction\"\n",
    ").show(10, truncate=50)\n",
    "\n",
    "# Cache predictions\n",
    "predictions_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49466d",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze prediction distribution\n",
    "print(\"Toxicity Level Distribution:\")\n",
    "predictions_df.groupBy(\"toxicity_level\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "predictions_df.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272aaae",
   "metadata": {},
   "source": [
    "## 9. User-Level Analysis\n",
    "\n",
    "Aggregate toxicity metrics at the user level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516c70a",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "from user_analysis import UserToxicityAnalyzer\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = UserToxicityAnalyzer()\n",
    "\n",
    "# Aggregate user toxicity\n",
    "user_aggregates = analyzer.aggregate_user_toxicity(predictions_df)\n",
    "\n",
    "print(\"âœ“ User-level aggregation completed\")\n",
    "print(\"\\nTop 10 Most Toxic Users:\")\n",
    "user_aggregates.select(\n",
    "    \"user_id\",\n",
    "    \"total_messages\",\n",
    "    \"avg_toxicity_score\",\n",
    "    \"max_toxicity_score\",\n",
    "    \"user_toxicity_level\",\n",
    "    \"toxic_messages_count\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41daa9a5",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Get user statistics\n",
    "user_stats = analyzer.get_user_statistics(user_aggregates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ade92",
   "metadata": {},
   "source": [
    "## 10. Visualizations\n",
    "\n",
    "Create comprehensive visualizations of toxicity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c42d2",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "user_agg_pd = user_aggregates.select(\n",
    "    \"user_id\",\n",
    "    \"total_messages\",\n",
    "    \"avg_toxicity_score\",\n",
    "    \"user_toxicity_level\",\n",
    "    \"toxic_messages_count\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"âœ“ Converted {len(user_agg_pd)} user records to Pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dc165",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 1: User Toxicity Level Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "level_order = ['MINIMAL', 'LOW', 'MODERATE', 'HIGH', 'VERY_HIGH']\n",
    "level_counts = user_agg_pd['user_toxicity_level'].value_counts()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=user_agg_pd, y='user_toxicity_level', order=level_order, palette='RdYlGn_r')\n",
    "plt.title('User Distribution by Toxicity Level', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Users', fontsize=11)\n",
    "plt.ylabel('Toxicity Level', fontsize=11)\n",
    "\n",
    "# Visualization 2: Average Toxicity Score Distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(user_agg_pd['avg_toxicity_score'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Moderate Threshold')\n",
    "plt.axvline(0.7, color='darkred', linestyle='--', linewidth=2, label='High Threshold')\n",
    "plt.title('Distribution of Average Toxicity Scores', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Toxicity Score', fontsize=11)\n",
    "plt.ylabel('Number of Users', fontsize=11)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425b853",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 3: Top 20 Most Toxic Users\n",
    "top_20_users = user_agg_pd.nlargest(20, 'avg_toxicity_score')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=top_20_users, y='user_id', x='avg_toxicity_score', palette='Reds_r')\n",
    "plt.title('Top 20 Most Toxic Users', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Average Toxicity Score', fontsize=12)\n",
    "plt.ylabel('User ID', fontsize=12)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Add score labels\n",
    "for i, v in enumerate(top_20_users['avg_toxicity_score']):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457a0ce",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 4: Messages vs Toxicity Score\n",
    "plt.figure(figsize=(12, 6))\n",
    "scatter_sample = user_agg_pd.sample(min(1000, len(user_agg_pd)))\n",
    "\n",
    "plt.scatter(\n",
    "    scatter_sample['total_messages'],\n",
    "    scatter_sample['avg_toxicity_score'],\n",
    "    c=scatter_sample['avg_toxicity_score'],\n",
    "    cmap='RdYlGn_r',\n",
    "    alpha=0.6,\n",
    "    s=100,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "plt.colorbar(label='Toxicity Score')\n",
    "plt.title('User Messages vs Average Toxicity Score', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Total Messages', fontsize=12)\n",
    "plt.ylabel('Average Toxicity Score', fontsize=12)\n",
    "plt.axhline(0.5, color='orange', linestyle='--', alpha=0.7, label='Moderate Threshold')\n",
    "plt.axhline(0.7, color='red', linestyle='--', alpha=0.7, label='High Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dda154",
   "metadata": {},
   "source": [
    "## 11. Export Results\n",
    "\n",
    "Save predictions and user aggregates to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f1370",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "# Export predictions\n",
    "output_predictions = \"output/toxicity_predictions.csv\"\n",
    "predictions_df.select(\n",
    "    \"id\",\n",
    "    substring(\"id\", 1, 8).alias(\"user_id\"),\n",
    "    \"comment_text\",\n",
    "    \"toxicity_score\",\n",
    "    \"toxicity_level\",\n",
    "    \"prediction\",\n",
    "    \"toxic\",\n",
    "    \"severe_toxic\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_hate\"\n",
    ").coalesce(1).write.mode(\"overwrite\").csv(output_predictions, header=True)\n",
    "\n",
    "print(f\"âœ“ Predictions saved to: {output_predictions}\")\n",
    "\n",
    "# Export user aggregates\n",
    "output_users = \"output/user_toxicity_levels.csv\"\n",
    "user_aggregates.coalesce(1).write.mode(\"overwrite\").csv(output_users, header=True)\n",
    "\n",
    "print(f\"âœ“ User toxicity levels saved to: {output_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a4d16",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = \"models/toxicity_model\"\n",
    "classifier.save_model(model_path)\n",
    "\n",
    "print(f\"âœ“ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92f336",
   "metadata": {},
   "source": [
    "## 12. Summary and Insights\n",
    "\n",
    "### Key Findings:\n",
    "1. **Model Performance**: Achieved strong performance with high accuracy and AUC scores\n",
    "2. **User Behavior**: Identified users with varying levels of toxicity\n",
    "3. **Toxicity Patterns**: Analyzed distribution of toxic content across the dataset\n",
    "\n",
    "### Next Steps:\n",
    "- Fine-tune model parameters for improved performance\n",
    "- Implement real-time toxicity detection\n",
    "- Add support for multi-class toxicity classification\n",
    "- Integrate with chat platforms for live monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**Project Completed Successfully! ðŸŽ‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88609edb",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Display final statistics\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL PROJECT STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Records Processed: {df_features.count()}\")\n",
    "print(f\"Model Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Total Users Analyzed: {user_stats['total_users']}\")\n",
    "print(f\"Overall Toxicity Rate: {user_stats['toxicity_rate']:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ“ DETOX PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nSpark Web UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0b598",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop the Spark session when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bc2fe",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to stop Spark session\n",
    "# spark.stop()\n",
    "# print(\"âœ“ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
